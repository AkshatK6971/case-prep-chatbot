# ğŸ’¼ Case Interview Preparation Chatbot

## ğŸ“ Introduction

The Case Interview Preparation Chatbot is an AI-powered assistant designed to help users prepare for consulting case interviews. Leveraging advanced language models, neural search, and retrieval-augmented generation (RAG), the chatbot provides tailored case questions, frameworks, feedback, and guidance using a rich database of real case resources and best practices.

## âœ¨ Features

- ğŸ—ï¸ **Case Generation:** Generate new, realistic case interview questions for practice.
- ğŸ§© **Case Solving Assistance:** Get step-by-step solutions, analysis, and structured answers to case problems.
- ğŸ—‚ï¸ **Framework Guidance:** Request and receive consulting frameworks and structured approaches for various case types.
- ğŸ“ **Feedback:** Receive actionable, detailed feedback on your case answers and performance.
- ğŸ” **Contextual Retrieval:** Uses Qdrant vector search to find and integrate relevant information from a curated set of casebooks and resources.
- ğŸ’¬ **Conversational Memory:** Maintains conversation history for context-aware responses.
- ğŸ–¥ï¸ **User-Friendly Interface:** Simple Streamlit frontend for interactive chat.

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/AkshatK6971/case-prep-chatbot.git
cd case-prep-chatbot
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment

```bash
python -m venv venv
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 3ï¸âƒ£ Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download and Start Qdrant (Vector Database)

The chatbot uses Qdrant for fast vector search. The easiest way to run Qdrant is via Docker:

```bash
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

- Qdrant will be available at `http://localhost:6333`.
- For more options, see the [Qdrant documentation](https://qdrant.tech/documentation/).

### 5ï¸âƒ£ Prepare the `.env` File

Create a `.env` file in the project root with the following content:

```bash
GROQ_KEY=your_groq_api_key_here
```

- Replace `your_groq_api_key_here` with your actual [Groq API key](https://console.groq.com/).

### 6ï¸âƒ£ Generate Embeddings and Upload to Qdrant

Run the following script to process the PDF resources, generate embeddings, and upload them to Qdrant:

```bash
python vectors.py
```

- This will create `embeddings.npy` and populate the Qdrant collection with vectorized document chunks.

### 7ï¸âƒ£ Launch the Streamlit Frontend

```bash
streamlit run frontend.py
```

- Open the provided local URL in your browser to start chatting with the assistant.

## ğŸ—‚ï¸ Project Structure

```bash
case_prep.py           # Core logic: RAG, intent detection, response generation
frontend.py            # Streamlit UI
vectors.py             # Embedding generation and Qdrant upload
embeddings.npy         # Saved embeddings (generated by vectors.py)
requirements.txt       # Python dependencies
case_prep_resources/   # PDF casebooks and resources
```

## â• Adding More Resources

- Place additional PDF casebooks in the `case_prep_resources/` folder and re-run `python vectors.py` to update the vector database.

## ğŸ“ Notes

- The chatbot uses HuggingFace's `all-MiniLM-L6-v2` model for embeddings.
- The language model is served via `Groq (Llama-3.3-70b-versatile)`.
- All data remains local except for API calls to Groq.

## ğŸ› ï¸ Troubleshooting

- Ensure Docker is running and Qdrant is accessible at `localhost:6333` before running the chatbot.
- If you add new PDFs, always re-run `vectors.py`.
- For API issues, check your `.env` and Groq key validity.
